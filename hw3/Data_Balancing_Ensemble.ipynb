{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-based (Ensemble) Models and Handling Imbalanced Data [30 points]\n",
    "\n",
    "For this problem, we will use the wine quality dataset on which the task is a binary classification of whether a given wine is of low or high quality based on different physicochemical features.\n",
    "Â \n",
    "The dataset consists of a set of physicochemical features as inputs and the target is wine quality stored in the target column, where a value of 1 corresponds to an instance of high quality wine and -1 corresponds to an instance of low quality ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data (3pts)\n",
    "Load the data from library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Use wine dataset from imlearn\n",
    "from imblearn.datasets import fetch_datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "datasets = fetch_datasets()\n",
    "\n",
    "# Wine quality dataset contains 12 features, descriptions found here: \n",
    "# https://archive.ics.uci.edu/ml/datasets/wine+quality\n",
    "data = datasets['wine_quality']\n",
    "\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Draw the class distribution of the dataset. What are possible problems if we train a classification model directly on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is imbalanced. Percenage of negative instances is much larger than that of positive instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of high quality observations 0.03736218864842793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23a935448d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries for plotting class distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "high_quality_ratio = len([i for i in y if i==1])/len(y)\n",
    "print('Percentage of high quality observations', high_quality_ratio)\n",
    "\n",
    "# color coding for 2 classes\n",
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "## code to plot the class distribution. Hint: countplot in seaborn\n",
    "sns.countplot(x=y,palette=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing a Random Forest classfier directly on the data (3pts)\n",
    "\n",
    "Let's first train a random forest classifier with default parameters using X_train and y_train and test the performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier accuray: 0.9510204081632653\n",
      "Random forest classifier recall: 0.10606060606060606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier # class for random forest classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "## Instantiate and fit a random forest classifier to the training data\n",
    "rf=RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "## Measure and print out the accuracy and recall\n",
    "test_acc = (y_test_pred==y_test).sum()/len(y_test)\n",
    "\n",
    "print('Random forest classifier accuray:', test_acc)\n",
    "\n",
    "test_recall = recall_score(y_test,y_test_pred)\n",
    "\n",
    "print('Random forest classifier recall:', test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quetion:** Compute the recall and accuracy scores of the random forest classifier. How is the gap between the accuracy and recall scores? Provide an explanation.\n",
    "\n",
    "The accuracy is high while the recall rate is very low.\n",
    "\n",
    "Resons:\n",
    "a) The default parameter of \"average\" in recall_score is \"binary\", which will only reports results for class specified by pos_lable, which is 1. \n",
    "b) The rate is low since the train data is imbanlanced and there are few positive samples, thus the model cannot predict positive labels well.\n",
    "c) While for accuracy, it's computed based on results of 2 classes, and positive class has very small proportion, so the lobal accuracy rate is really high and the \"binary\" recall rate for positive labels is really low.\n",
    "d) If we use average='weighted', than the gap can be eliminated since it's computed on global view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data balancing via Smote (6pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of high quality counts in the balanced data:50.0%\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE #Over sampling\n",
    "import numpy as np\n",
    "\n",
    "## Instantiate smote and balance training data only\n",
    "sm=SMOTE(random_state=42)\n",
    "X_train_res,y_train_res=sm.fit_resample(X_train,y_train)\n",
    "## Compute and print percentage of high quality wine after balancing\n",
    "print('Percentage of high quality counts in the balanced data:{}%'.format(sum(y_train_res==1)*100/len(y_train_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Plot the distribution of balanced training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23a957a5978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "## plot the class distribution of training data after balanced\n",
    "sns.countplot(x=y_train_res,palette=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrain and test our random forest model on the balanced training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier accuray: 0.9469387755102041\n",
      "Random forest classifier recall: 0.3181818181818182\n"
     ]
    }
   ],
   "source": [
    "## Instantiate random forest and train on balanced training data\n",
    "# X_train_res, X_test_res, y_train_res, y_test_res = train_test_split(X_res, y_res, random_state=0)\n",
    "rf2=RandomForestClassifier(random_state=42)\n",
    "rf2.fit(X_train_res, y_train_res)\n",
    "y_test_pred=rf2.predict(X_test)\n",
    "test_acc = sum(y_test_pred==y_test)/len(y_test)\n",
    "\n",
    "print('Random forest classifier accuray:', test_acc)\n",
    "\n",
    "test_recall = recall_score(y_test,y_test_pred)\n",
    "\n",
    "print('Random forest classifier recall:', test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP is: 21\n",
      "FN is: 45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3181818181818182"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "TP=sum(y_test[y_test_pred==1]==1)\n",
    "print('TP is:',TP)\n",
    "FN=sum(y_test[y_test_pred==-1]==1)\n",
    "print('FN is:',FN)\n",
    "TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Compute the recall and accuracy scores of the new random forest classifier. How do the accuracy and recall change compared to those without data balancing?\n",
    "\n",
    "a) The accuracy decrease a bit since the threshold of positive labels is lower and will have more false positive cases\n",
    "b) Recall score increase since the training samples with positive labels increase and the model now can have better prediction for positive labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control complexity of the model (18pts)\n",
    "\n",
    "#### Control the depth of decision trees in our ensemble (6pts)\n",
    "By default, the decision trees in random forest are expanded until all leaves are pure or until all leaves contain less than a certain number set by min_samples_split parameter. Let's try a fixed maximum depth that the tree can expand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier accuray: 0.8448979591836735\n",
      "Random forest classifier recall: 0.5757575757575758\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model with max depth trees being 3\n",
    "rf3=RandomForestClassifier(max_depth=3,random_state=42)\n",
    "rf3.fit(X_train_res, y_train_res)\n",
    "y_test_pred=rf3.predict(X_test)\n",
    "test_acc = sum(y_test_pred==y_test)/len(y_test)\n",
    "\n",
    "print('Random forest classifier accuray:', test_acc)\n",
    "\n",
    "test_recall = recall_score(y_test,y_test_pred)\n",
    "\n",
    "print('Random forest classifier recall:', test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Compute the recall and accuracy scores of the new random forest classifier. How do the accuracy and recall change compared to those in the default parameter case?\n",
    "\n",
    "The accuracy is lower since the max depth is fixed and may lead to underfitting.\n",
    "The recall is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP is: 38\n",
      "FN is: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5757575757575758"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "TP=sum(y_test[y_test_pred==1]==1)\n",
    "print('TP is:',TP)\n",
    "FN=sum(y_test[y_test_pred==-1]==1)\n",
    "print('FN is:',FN)\n",
    "TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the number of trees in the forest (6pts)\n",
    "By default, we use 10 random trees. Let's increase this number to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier accuray: 0.8318367346938775\n",
      "Random forest classifier recall: 0.5454545454545454\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model with max depth of 3 and 100 decision trees\n",
    "rf4=RandomForestClassifier(max_depth=3,random_state=42,n_estimators=1000)\n",
    "rf4.fit(X_train_res, y_train_res)\n",
    "y_test_pred=rf4.predict(X_test)\n",
    "test_acc = sum(y_test_pred==y_test)/len(y_test)\n",
    "\n",
    "print('Random forest classifier accuray:', test_acc)\n",
    "\n",
    "test_recall = recall_score(y_test,y_test_pred)\n",
    "\n",
    "print('Random forest classifier recall:', test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Compute the recall and accuracy scores of the random forest classifier. How do the accuracy and recall change compared to those with 10 trees? What do the results imply about increasing the number of trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Since the default n_estimators is 100 rather than 10, which is the default value mentioned in the question, so here I use n_estimators=1000 to compare.\n",
    "The accuracy and recall both decrease, maybe too many trees cause overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree pruning by min_impurity_decrease (6pts)\n",
    "By default, the tree keeps expanding until the impurity is 0. However, we can specify a minimum impurity decrease amount under which nodes in the tree stop branching. RandomForestClassifier in sklearn use min_impurity_decrease for setting this threshold. Let's try that on our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest classifier accuray: 0.9151020408163265\n",
      "Random forest classifier recall: 0.4696969696969697\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model with min impurity decrease of 0.001\n",
    "rf5=RandomForestClassifier(random_state=42,min_impurity_decrease=0.001)\n",
    "rf5.fit(X_train_res, y_train_res)\n",
    "y_test_pred=rf5.predict(X_test)\n",
    "test_acc = sum(y_test_pred==y_test)/len(y_test)\n",
    "\n",
    "print('Random forest classifier accuray:', test_acc)\n",
    "\n",
    "test_recall = recall_score(y_test,y_test_pred)\n",
    "\n",
    "print('Random forest classifier recall:', test_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Compute the recall and accuracy scores of the random forest classifier. How does the recall change compared to those with 10 trees and max_depth = 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy increase and recall decrease. Since seting a minimum impurity can prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "5yX9Z",
   "launcher_item_id": "eqnV3",
   "part_id": "Msnj0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
